# 进一步提升TSDB服务端性能的研究

## 1. 发现grpc 的 `BlockingStub` 限制了服务端性能

按照之前的思路，即让多个客户端向服务端的同一个`batch` 中写入监控数据以减少`batch`批量提交所需的等待时间，加上让服务端并行运行多个`batch`的思路，我重构了代码。

首先我写了`ClientManager`类控制多个客户端并行运行，向服务端提交数据（按照之前的`BlockingStub`方式），并做了测试。  

测试表明，随着客户端数量增加，服务端接收监控数据的效率的确有所提升，但最终卡在瓶颈：6000～7000 条每秒。   
有这样的瓶颈的话，效率自然没法更高了。   

于是我想，这种`BlockingStub`方式让客户端与服务端“一问一答”，似乎有些低效，限制了服务端接收数据的效率。  
 
然后我回想到grpc还提供客户端与服务端以流的形式进行通讯，这应该会高效一些。

## 2. 使用 `客户端流式rpc` 有助于提升性能
我再次修改代码，把客户端与服务端的通信方式换成`客户端流式rpc`，即客户端持有一个到服务端的连接，  
通过这个连接向服务端连续发送多条数据以后，再把连接关闭。  

经过测试，这比之前的”一问一答“的通信方式要高效很多： 服务端接收数据达到20000 条每秒  
（这只是数据接收，还未插入数据库）。  

但随着客户端数量的增多，服务端接收数据效率在下降，这应该是进程切换造成的花费，所以20000 条每秒似乎是我的电脑所能达到的极限了。

## 3. 维护 `大小、数量合适的batch` 有助于提升性能
在采用`客户端流式rpc`的通信方式的基础上，我调整了服务端所维护的`batch`的数量和大小，并一一进行了测试。  

测试结果表明，让服务端维护 `大小、数量合适的batch` 有助于提升性能，我的电脑在有2个`batch`、每个`batch`达到100000 条数据后提交的设定下，服务端达到了最高性能：  

监控信息接收及记录达到16000 条每秒。

## 4. 探究小结

在学透、用好现有模块的基础上才谈得上改进自己的设计。  

在TSDB中就涉及`client`、`server` 和 `DBMS` 这三大模块，得先了解到这三大模块之间通信的最高效方式，  
才能使整个系统的性能达到最优。  
在 `client` 和 `server` 之间我采用了`客户端流式rpc`，在`server` 和 `DBMS` 我采用了`batch`。  

现在我的服务端性能达到 16000 条每秒，在原有基础上提升了`50%`。

